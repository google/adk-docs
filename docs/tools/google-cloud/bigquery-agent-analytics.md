# BigQuery Agent Analytics Plugin

<div class="language-support-tag">
  <span class="lst-supported">Supported in ADK</span><span class="lst-python">Python v1.21.0</span><span class="lst-preview">Preview</span>
</div>

The BigQuery Agent Analytics Plugin (v2.0) significantly enhances the Agent Development Kit (ADK) by providing a robust, high-throughput solution for in-depth agent behavior analysis. Using the ADK Plugin architecture and the BigQuery Write API, it captures and logs critical operational events directly into a Google BigQuery table, empowering you with advanced capabilities for debugging, real-time monitoring, and comprehensive offline performance evaluation.

This version introduces several key improvements:
- **High-throughput streaming:** Leverages the BigQuery Write API for asynchronous and efficient data ingestion.
- **Rich, structured data:** Logs event payloads as JSON objects instead of pre-formatted strings, allowing for easier and more powerful queries.
- **Multi-modal content support:** Can log complex, multi-modal content, including text, images, and other file types.
- **GCS Offloading:** Automatically offloads large content parts to a specified Google Cloud Storage bucket to keep your BigQuery table lean and cost-effective.
- **Enhanced tracing:** Captures OpenTelemetry-style trace and span IDs to help you reconstruct the exact sequence of operations within an agent invocation.

!!! example "Preview release"

    The BigQuery Agent Analytics Plugin is in Preview release. For more
    information, see the
    [launch stage descriptions](https://cloud.google.com/products#product-launch-stages).

!!! warning "BigQuery Storage Write API"

    This feature uses the **BigQuery Storage Write API**, which is a paid service.
    For information on costs, see the
    [BigQuery documentation](https://cloud.google.com/bigquery/pricing?e=48754805&hl=en#data-ingestion-pricing).

## Use cases

-   **Agent workflow debugging and analysis:** Capture a wide range of
    *plugin lifecycle events* (LLM calls, tool usage) and *agent-yielded
    events* (user input, model responses), into a well-defined schema.
-   **High-volume analysis and debugging:** Logging operations are performed
    asynchronously in a separate thread to avoid blocking the main agent
    execution. Designed to handle high event volumes, the plugin preserves
    event order via timestamps.
- **Performance monitoring:** Analyze latency for individual agent steps (LLM calls, tool execution) and time-to-first-token for model responses.
- **Multi-modal agent debugging:** Inspect both the summary and the individual parts of multi-modal content sent to or generated by your agent.

## Prerequisites

-   **Google Cloud Project** with the **BigQuery API** enabled.
-   **BigQuery Dataset:** Create a dataset to store logging tables before
    using the plugin. The plugin automatically creates the necessary events table within the dataset if the table does not exist. By default, this table is named `agent_events_v2`.
-   **Authentication:**
    -   **Local:** Run `gcloud auth application-default login`.
    -   **Cloud:** Ensure your service account has the required permissions.

### IAM permissions

For the agent to work properly, the principal (e.g., service account, user account) under which the agent is running needs these Google Cloud roles:
*   `roles/bigquery.jobUser` at the Project Level to run BigQuery queries in your project.
*   `roles/bigquery.dataEditor` at the Table Level to write log/event data to your BigQuery Table.
* If you want the plugin to automatically create the table, you need to grant `roles/bigquery.dataEditor` on the BigQuery dataset where the table will be created.
* If you use the GCS offloading feature, the principal also needs the `roles/storage.objectCreator` role on the specified GCS bucket.

## Use with agent

You use the BigQuery Analytics Plugin by configuring and registering it with
your ADK agent's `App` object. The following example shows a typical implementation.

```python title="my_bq_agent/agent.py"
# my_bq_agent/agent.py
import os
from google.adk.apps import App
from google.adk.plugins.bigquery_agent_analytics_plugin import (
    BigQueryAgentAnalyticsPlugin,
    BigQueryLoggerConfig,
)
from google.adk.agents import Agent
from google.adk.models.google_llm import Gemini

# --- Configuration ---
PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT", "your-gcp-project-id")
DATASET_ID = os.environ.get("BIG_QUERY_DATASET_ID", "your-big-query-dataset-id")
LOCATION = os.environ.get("GOOGLE_CLOUD_LOCATION", "your-gcp-project-location")

if PROJECT_ID == "your-gcp-project-id":
    raise ValueError("Please set GOOGLE_CLOUD_PROJECT or update the code.")
if DATASET_ID == "your-big-query-dataset-id":
    raise ValueError("Please set BIG_QUERY_DATASET_ID or update the code.")
if LOCATION == "your-gcp-project-location":
    raise ValueError("Please set GOOGLE_CLOUD_LOCATION or update the code.")

# --- CRITICAL: Set environment variables BEFORE Gemini instantiation ---
os.environ["GOOGLE_CLOUD_PROJECT"] = PROJECT_ID
os.environ["GOOGLE_CLOUD_LOCATION"] = LOCATION
os.environ["GOOGLE_GENAI_USE_VERTEXAI"] = "True" # Make sure you have Vertex AI API enabled

# --- Initialize the Plugin ---
# The plugin can be configured with a BigQueryLoggerConfig object.
bq_logging_plugin = BigQueryAgentAnalyticsPlugin(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    location=LOCATION,
    config=BigQueryLoggerConfig(
        table_id="agent_events_v2",
        # Optional: Offload large content to a GCS bucket
        # gcs_bucket_name="your-gcs-bucket-for-offloading"
    )
)

# --- Initialize Model ---
llm = Gemini(model="gemini-1.5-flash")

root_agent = Agent(
    model=llm,
    name="my_bq_agent",
    instruction="You are a helpful assistant.",
)

# --- Create the App ---
app = App(
    name="my_bq_agent",
    root_agent=root_agent,
    plugins=[bq_logging_plugin], # Register the plugin here
)
```

### Run and test agent

Test the plugin by running the agent and making a few requests through the chat
interface. These actions create events which are recorded in your Google Cloud project BigQuery instance. Once these events have
been processed, you can view the data for them in the [BigQuery Console](https://console.cloud.google.com/bigquery), using a query like this:

```sql
SELECT
  timestamp,
  event_type,
  agent,
  content
FROM
  `your-gcp-project-id.your-big-query-dataset-id.agent_events_v2`
ORDER BY
  timestamp DESC
LIMIT 20;
```

## Configuration options

You can customize the plugin using the `BigQueryLoggerConfig` dataclass.

-   **`enabled`** (`bool`, default: `True`): To disable the plugin from logging agent data to the BigQuery table, set this parameter to `False`.
-   **`table_id`** (`str`, default: `"agent_events_v2"`): The ID of the BigQuery table to store the logs.
-   **`event_allowlist`** (`Optional[List[str]]`, default: `None`): A list of event types to log. If `None`, all events are logged except those in `event_denylist`.
-   **`event_denylist`** (`Optional[List[str]]`, default: `None`): A list of event types to skip logging.
-   **`max_content_length`** (`int`, default: `512000`): The maximum length of text content (in bytes) before truncation or offloading.
-   **`gcs_bucket_name`** (`Optional[str]`, default: `None`): If provided, large content (images, audio, video, or text exceeding the size threshold) will be offloaded to this GCS bucket.
-   **`log_multi_modal_content`** (`bool`, default: `True`): Whether to log detailed structured information about multi-modal content parts in the `content_parts` column.
-   **`batch_size`** (`int`, default: `1`): The number of rows to buffer in memory before sending a batch to BigQuery.
-   **`batch_flush_interval`** (`float`, default: `1.0`): The maximum time (in seconds) to wait before flushing a batch, even if `batch_size` has not been reached.
-   **`shutdown_timeout`** (`float`, default: `10.0`): Seconds to wait for logs to flush during shutdown.
-   **`queue_max_size`** (`int`, default: `10000`): The maximum number of events to hold in the in-memory queue before dropping new events.
-   **`retry_config`** (`RetryConfig`, default: `RetryConfig()`): Configuration for retrying failed write attempts to BigQuery.
-   **`content_formatter`** (`Optional[Callable[[Any, str], Any]]`, default: `None`): An optional function to transform or redact event content before it is logged.

The following code sample shows how to define a more complex configuration for the plugin:

```python
import json
from typing import Any
from google.adk.plugins.bigquery_agent_analytics_plugin import BigQueryLoggerConfig

# Example content formatter to redact sensitive information
def redact_sensitive_info(content: Any, event_type: str) -> Any:
    """
    A custom formatter that redacts a specific 'ssn' field from a JSON dict
    and returns the content as a dict.
    """
    if isinstance(content, dict) and "args" in content:
        if "ssn" in content["args"]:
            content["args"]["ssn"] = "[REDACTED]"
    return content

config = BigQueryLoggerConfig(
    enabled=True,
    table_id="my_custom_agent_logs",
    event_allowlist=["LLM_REQUEST", "LLM_RESPONSE", "TOOL_COMPLETED"], # Only log these events
    batch_size=10,
    batch_flush_interval=5.0,
    gcs_bucket_name="my-adk-logs-bucket", # Offload large content
    content_formatter=redact_sensitive_info, # Redact sensitive data
)

plugin = BigQueryAgentAnalyticsPlugin(
    project_id="your-gcp-project-id",
    dataset_id="your-big-query-dataset-id",
    config=config
)
```

## Schema and production setup

The plugin automatically creates the table if it does not exist. However, for
production, we recommend creating the table manually with **partitioning** and
**clustering** for performance and cost optimization.

**Recommended DDL:**

```sql
CREATE TABLE `your-gcp-project-id.your_dataset_id.agent_events_v2` (
  timestamp TIMESTAMP NOT NULL OPTIONS(description="The UTC timestamp when the event occurred."),
  event_type STRING OPTIONS(description="The category of the event (e.g., 'LLM_REQUEST', 'TOOL_CALL')."),
  agent STRING OPTIONS(description="The name of the agent that generated this event."),
  session_id STRING OPTIONS(description="A unique identifier for the entire conversation session."),
  invocation_id STRING OPTIONS(description="A unique identifier for a single turn or execution within a session."),
  user_id STRING OPTIONS(description="The identifier of the end-user participating in the session, if available."),
  trace_id STRING OPTIONS(description="OpenTelemetry trace ID for distributed tracing across services."),
  span_id STRING OPTIONS(description="OpenTelemetry span ID for this specific operation."),
  parent_span_id STRING OPTIONS(description="OpenTelemetry parent span ID to reconstruct the operation hierarchy."),
  content JSON OPTIONS(description="The primary payload of the event, stored as a JSON object."),
  content_parts ARRAY<STRUCT<
    mime_type STRING,
    uri STRING,
    object_ref STRUCT<
        uri STRING,
        version STRING,
        authorizer STRING,
        details JSON
    >,
    text STRING,
    part_index INT64,
    part_attributes STRING,
    storage_mode STRING
  >> OPTIONS(description="For multi-modal events, contains a list of content parts (text, images, etc.)."),
  attributes JSON OPTIONS(description="A JSON object for additional event metadata."),
  latency_ms JSON OPTIONS(description="A JSON object with latency measurements, like 'total_ms' and 'time_to_first_token_ms'."),
  status STRING OPTIONS(description="The outcome of the event, typically 'OK' or 'ERROR'."),
  error_message STRING OPTIONS(description="Detailed error message if the status is 'ERROR'."),
  is_truncated BOOLEAN OPTIONS(description="Boolean flag indicating if the 'content' field was truncated.")
)
PARTITION BY DATE(timestamp)
CLUSTER BY event_type, agent, user_id;
```

### Event content structure

The `content` column contains a JSON object whose structure depends on the `event_type`. This allows you to run powerful, targeted queries on the event data.

-   **`LLM_REQUEST`**:
    -   `prompt`: An array of messages representing the conversation history.
    -   `system_prompt`: The system instruction provided to the model.
-   **`LLM_RESPONSE`**:
    -   `response`: The string content of the model's response.
    -   `usage`: A JSON object with token counts (`prompt`, `completion`, `total`).
-   **`TOOL_STARTING`**:
    -   `tool`: The name of the tool being called.
    -   `args`: A JSON object of the arguments passed to the tool.
-   **`TOOL_COMPLETED`**:
    -   `tool`: The name of the tool that was called.
    -   `result`: A JSON object of the result returned by the tool.
-   **`USER_MESSAGE_RECEIVED`**:
    - `text_summary`: A string summary of the user's message. For multi-modal input, detailed parts are in `content_parts`.

## Advanced analysis queries

The structured JSON `content` and new metadata fields enable more precise and powerful analytics queries.

Before executing these queries, ensure you update the GCP project ID, BigQuery dataset ID, and the table ID (`agent_events_v2` by default).

**Trace a specific conversation turn**

```sql
SELECT
  timestamp,
  event_type,
  agent,
  content,
  latency_ms
FROM
  `your-gcp-project-id.your-dataset-id.agent_events_v2`
WHERE
  invocation_id = 'your-invocation-id'
ORDER BY
  timestamp ASC;
```

**Daily invocation volume**

```sql
SELECT
  DATE(timestamp) as log_date,
  COUNT(DISTINCT invocation_id) as count
FROM
  `your-gcp-project-id.your-dataset-id.agent_events_v2`
WHERE
  event_type = 'INVOCATION_STARTING'
GROUP BY
  log_date
ORDER BY
  log_date DESC;
```

**Token usage analysis**

Analyze the average token usage for successful LLM responses.

```sql
SELECT
  AVG(JSON_VALUE(content, '$.usage.total')) as avg_total_tokens,
  AVG(JSON_VALUE(content, '$.usage.prompt')) as avg_prompt_tokens,
  AVG(JSON_VALUE(content, '$.usage.completion')) as avg_completion_tokens
FROM
  `your-gcp-project-id.your-dataset-id.agent_events_v2`
WHERE
  event_type = 'LLM_RESPONSE' AND status = 'OK';
```

**Find tool calls with specific arguments**

Find all calls to a specific tool where a certain argument was used.

```sql
SELECT
  timestamp,
  invocation_id,
  JSON_QUERY(content, '$.args') as tool_arguments
FROM
  `your-g-project-id.your-dataset-id.agent_events_v2`
WHERE
  event_type = 'TOOL_STARTING'
  AND JSON_VALUE(content, '$.tool') = 'your_tool_name'
  AND JSON_VALUE(content, '$.args.your_argument_name') = 'some_value';

```

**Error monitoring**

```sql
SELECT
  timestamp,
  event_type,
  error_message,
  agent,
  invocation_id
FROM
  `your-gcp-project-id.your-dataset-id.agent_events_v2`
WHERE
  status = 'ERROR'
ORDER BY
  timestamp DESC
LIMIT 50;
```

## Additional resources

-   [BigQuery Write API](https://cloud.google.com/bigquery/docs/write-api)
-   [Querying JSON data in BigQuery](https://cloud.google.com/bigquery/docs/reference/standard-sql/json-functions)
-   [BigQuery product documentation](https://cloud.google.com/bigquery/docs)
