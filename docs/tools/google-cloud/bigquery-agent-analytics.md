# BigQuery Agent Analytics Plugin

<div class="language-support-tag">
  <span class="lst-supported">Supported in ADK</span><span class="lst-python">Python v1.21.0</span><span class="lst-preview">Preview</span>
</div>

The BigQuery Agent Analytics Plugin (v2.0) significantly enhances the Agent Development Kit (ADK) by providing a robust, scalable, and feature-rich solution for in-depth agent behavior analysis. Using the high-throughput BigQuery Storage Write API, it captures and logs critical operational events directly into a Google BigQuery table. This empowers you with advanced capabilities for debugging, real-time monitoring, and comprehensive offline performance evaluation.

Version 2.0 is a complete rewrite of the plugin, introducing major improvements:

- **High-Throughput Ingestion:** Now uses the BigQuery Write API for asynchronous, streaming ingestion, making it suitable for high-volume production environments.
- **Multi-Modal Content Logging:** Can now log complex, multi-modal inputs and outputs, with capabilities to offload large content (like images, audio, video) to Google Cloud Storage (GCS).
- **Richer Data Schema:** The BigQuery table schema is redesigned to be more structured, using `JSON` data types and nested fields to capture more detailed event data, including OpenTelemetry-style tracing (`trace_id`, `span_id`) and detailed latency metrics.
- **Enhanced Configuration:** Offers more granular control over batching, retries, content handling, and more.

!!! example "Preview release"

    The BigQuery Agent Analytics Plugin is in Preview release. For more
    information, see the
    [launch stage descriptions](https://cloud.google.com/products#product-launch-stages).

!!! warning "BigQuery Storage Write API"

    This feature uses **BigQuery Storage Write API**, which is a paid service.
    For information on costs, see the
    [BigQuery documentation](https://cloud.google.com/bigquery/pricing?e=48754805&hl=en#data-ingestion-pricing).

## Use cases

-   **Agent workflow debugging and analysis:** Capture a wide range of
    *plugin lifecycle events* (LLM calls, tool usage) and *agent-yielded
    events* (user input, model responses), into a well-defined schema.
-   **High-volume analysis and debugging:** Logging operations are performed
    asynchronously in a separate thread to avoid blocking the main agent
    execution. Designed to handle high event volumes, the plugin preserves
    event order via timestamps.
-   **Multi-modal Agent Analysis:** Track and store not just text, but also images, audio, and other binary data used or generated by your agent by offloading them to Google Cloud Storage.

## Prerequisites

-   **Google Cloud Project** with the **BigQuery API** and **BigQuery Storage Write API** enabled.
-   **BigQuery Dataset:** Create a dataset to store logging tables before
    using the plugin. The plugin automatically would create the necessary events table within the dataset if the table does not exist.
-   **Authentication:**
    -   **Local:** Run `gcloud auth application-default login`.
    -   **Cloud:** Ensure your service account has the required permissions.

### IAM permissions

For the agent to work properly, the principal (e.g., service account, user account) under which the agent is running needs these Google Cloud roles:
*   `roles/bigquery.jobUser` at the Project Level to run BigQuery jobs.
*   `roles/bigquery.dataEditor` at the Table or Dataset Level to write event data.
*   If you enable GCS offloading, the principal also needs `roles/storage.objectCreator` on the target GCS Bucket.

## Use with agent

You use the BigQuery Analytics Plugin by configuring and registering it with
your ADK agent's `App` object. The following example shows a typical implementation.

```python title="my_bq_agent/agent.py"
# my_bq_agent/agent.py
import os
import google.auth
from google.adk.apps import App
from google.adk.plugins.bigquery_agent_analytics_plugin import BigQueryAgentAnalyticsPlugin, BigQueryLoggerConfig
from google.adk.agents import Agent
from google.adk.models.google_llm import Gemini

# --- Configuration ---

PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT", "your-gcp-project-id")
DATASET_ID = os.environ.get("BIG_QUERY_DATASET_ID", "your-big-query-dataset-id")
LOCATION = os.environ.get("GOOGLE_CLOUD_LOCATION", "us-central1") # The location of your dataset
GCS_BUCKET = os.environ.get("GCS_BUCKET_FOR_OFFLOAD", "your-gcs-bucket-name") # Optional: for multi-modal

if PROJECT_ID == "your-gcp-project-id":
    raise ValueError("Please set GOOGLE_CLOUD_PROJECT or update the code.")
if DATASET_ID == "your-big-query-dataset-id":
    raise ValueError("Please set BIG_QUERY_DATASET_ID or update the code.")

# --- CRITICAL: Set environment variables BEFORE Gemini instantiation ---

os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID
os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION
os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'True' # Make sure you have Vertex AI API enabled

# --- Initialize the Plugin ---

# Configure the plugin for GCS offloading and specify a new table name
bq_plugin_config = BigQueryLoggerConfig(
    table_id="my_agent_events_v2",
    gcs_bucket_name=GCS_BUCKET,
    log_multi_modal_content=True,
    batch_size=5, # Write to BigQuery in batches of 5 events
    batch_flush_interval=2.0 # or every 2 seconds
)

bq_logging_plugin = BigQueryAgentAnalyticsPlugin(
    project_id=PROJECT_ID,
    dataset_id=DATASET_ID,
    config=bq_plugin_config,
    location=LOCATION
)

# --- Initialize Model and Agent ---

llm = Gemini(model="gemini-1.5-flash-001")

root_agent = Agent(
    model=llm,
    name='my_bq_agent',
    instruction="You are a helpful assistant.",
)

# --- Create the App ---

app = App(
    name="my_bq_agent",
    root_agent=root_agent,
    plugins=[bq_logging_plugin], # Register the plugin here
)
```

### Run and test agent

Run your agent and make a few requests. These actions create events which are
recorded in your BigQuery table. You can then query the data in the [BigQuery Console](https://console.cloud.google.com/bigquery).

```sql
SELECT
  timestamp,
  event_type,
  JSON_QUERY(content, '$') AS content_json,
  status,
  error_message
FROM
  `your-gcp-project-id.your-big-query-dataset-id.my_agent_events_v2`
ORDER BY
  timestamp DESC
LIMIT 20;

```

## Configuration options

You can customize the plugin by passing a `BigQueryLoggerConfig` object during initialization.

- **`enabled`** (`bool`, default: `True`): Enable or disable the plugin.
- **`event_allowlist`** (`Optional[List[str]]`, default: `None`): If set, only log events of these types.
- **`event_denylist`** (`Optional[List[str]]`, default: `None`): If set, skip logging for these event types.
- **`max_content_length`** (`int`, default: `512000`): The maximum length in characters for inline text content before truncation or offloading.
- **`table_id`** (`str`, default: `agent_events_v2`): The ID of the BigQuery table to log events to.
- **`clustering_fields`** (`List[str]`, default: `["event_type", "agent", "user_id"]`): A list of fields to use for clustering the BigQuery table, which can improve query performance and reduce costs.
- **`log_multi_modal_content`** (`bool`, default: `True`): If `True`, logs detailed information about multi-modal content parts into the `content_parts` field.
- **`gcs_bucket_name`** (`Optional[str]`, default: `None`): If provided, large content parts (like images, audio, or long text) will be offloaded to this Google Cloud Storage bucket.
- **`connection_id`** (`Optional[str]`, default: `None`): If provided, this connection ID will be used as the authorizer for ObjectRef columns when offloading to GCS. Format: `"location.connection_id"`.
- **`batch_size`** (`int`, default: `1`): The number of event rows to batch together before writing to BigQuery. Increasing this can improve ingestion performance.
- **`batch_flush_interval`** (`float`, default: `1.0`): The maximum time in seconds to wait before flushing a batch, even if it hasn't reached `batch_size`.
- **`shutdown_timeout`** (`float`, default: `10.0`): The number of seconds to wait for the event queue to drain during a graceful shutdown.
- **`queue_max_size`** (`int`, default: `10000`): The maximum number of events to hold in the in-memory queue before dropping new events.
- **`retry_config`** (`RetryConfig`, default: `RetryConfig()`): Configuration for the retry mechanism on failed writes. See the source code for `RetryConfig` attributes.
- **`content_formatter`** (`Optional[Callable]`, default: `None`): A custom function to format or redact the `content` payload before logging.

## Schema and production setup

The plugin automatically creates the table if it does not exist using a default schema. For production environments, we strongly recommend creating the table manually with **partitioning** and **clustering** to optimize performance and costs.

**Recommended DDL:**

```sql
CREATE TABLE `your-gcp-project-id.your-dataset-id.agent_events_v2` (
  timestamp TIMESTAMP NOT NULL OPTIONS(description="The UTC timestamp when the event occurred."),
  event_type STRING OPTIONS(description="The category of the event (e.g., 'LLM_REQUEST', 'TOOL_CALL')."),
  agent STRING OPTIONS(description="The name of the agent that generated this event."),
  session_id STRING OPTIONS(description="A unique identifier for the entire conversation session."),
  invocation_id STRING OPTIONS(description="A unique identifier for a single turn or execution within a session."),
  user_id STRING OPTIONS(description="The identifier of the end-user participating in the session, if available."),
  trace_id STRING OPTIONS(description="OpenTelemetry trace ID for distributed tracing."),
  span_id STRING OPTIONS(description="OpenTelemetry span ID for this specific operation."),
  parent_span_id STRING OPTIONS(description="OpenTelemetry parent span ID to reconstruct the operation hierarchy."),
  content JSON OPTIONS(description="The primary payload of the event, stored as a JSON object."),
  content_parts ARRAY<STRUCT<
    mime_type STRING,
    uri STRING,
    object_ref STRUCT<uri STRING, version STRING, authorizer STRING, details JSON>,
    text STRING,
    part_index INT64,
    part_attributes STRING,
    storage_mode STRING
  >> OPTIONS(description="For multi-modal events, contains a list of content parts (text, images, etc.)."),
  attributes JSON OPTIONS(description="A JSON object for additional event metadata."),
  latency_ms JSON OPTIONS(description="A JSON object containing latency measurements, such as 'total_ms' and 'time_to_first_token_ms'."),
  status STRING OPTIONS(description="The outcome of the event, typically 'OK' or 'ERROR'."),
  error_message STRING OPTIONS(description="Detailed error message if the status is 'ERROR'."),
  is_truncated BOOL OPTIONS(description="Boolean flag indicating if any content was truncated.")
)
PARTITION BY DATE(timestamp)
CLUSTER BY event_type, agent, user_id;
```

## Event Data Structure

The `content` and `attributes` columns are now `JSON` fields, providing a more structured way to store event data. The format of the `content` JSON object varies by `event_type`.

#### LLM_REQUEST

- **content:** `{"prompt": [{"role": "user", "content": "..."}], "system_prompt": "..."}`
- **attributes:** `{"llm_config": {"temperature": 1.0, ...}, "tools": ["tool_name_1"]}`

#### LLM_RESPONSE

- **content:** `{"response": "text: 'Here is the data.'", "usage": {"prompt": 10, "completion": 5, "total": 15}}`

#### TOOL_STARTING

- **content:** `{"tool": "list_datasets", "args": {"project_id": "my-project"}}`

#### TOOL_COMPLETED

- **content:** `{"tool": "list_datasets", "result": ["dataset_1", "dataset_2"]}`

#### Multi-Modal Data (`content_parts`)

When `log_multi_modal_content` is `True`, detailed information about each part of a multi-modal request or response is stored in the `content_parts` array.

- **`storage_mode`** indicates how the part is stored:
    - `INLINE`: The content is stored directly in the `text` field.
    - `GCS_REFERENCE`: The content was offloaded to GCS, and the `uri` field contains the `gs://` path.
    - `EXTERNAL_URI`: The content was already a URI provided by the user.
- **`object_ref`** contains a structured reference to the GCS object that can be queried directly if you have the correct permissions.

## Advanced analysis queries

The following examples demonstrate how to query the new `JSON`-based schema.

**Trace a specific conversation turn**

```sql
SELECT
  timestamp,
  event_type,
  agent,
  JSON_QUERY(content) AS content_payload,
  status
FROM
  `your-gcp-project-id.your-dataset-id.agent_events_v2`
WHERE
  invocation_id = 'your-invocation-id'
ORDER BY
  timestamp ASC;

```

**Daily invocation volume**

```sql
SELECT
  DATE(timestamp) as log_date,
  COUNT(DISTINCT invocation_id) as count
FROM
  `your-gcp-project-id.your-dataset-id.agent_events_v2`
WHERE
  event_type = 'INVOCATION_STARTING'
GROUP BY
  log_date
ORDER BY
  log_date DESC;

```

**Average Token Usage**

Extract total tokens from the `content` JSON payload for `LLM_RESPONSE` events.

```sql
SELECT
  AVG(CAST(JSON_VALUE(content, '$.usage.total') AS INT64)) as avg_total_tokens
FROM
  `your-gcp-project-id.your-dataset-id.agent_events_v2`
WHERE
  event_type = 'LLM_RESPONSE'
  AND JSON_VALUE(content, '$.usage.total') IS NOT NULL;
```

**Find Offloaded Content**

Query the `content_parts` array to find all events where content was offloaded to GCS.

```sql
SELECT
  timestamp,
  event_type,
  invocation_id,
  part.uri AS gcs_uri
FROM
  `your-gcp-project-id.your-dataset-id.agent_events_v2`,
  UNNEST(content_parts) AS part
WHERE
  part.storage_mode = 'GCS_REFERENCE';
```

## Additional resources

-   [BigQuery Storage Write API](https://cloud.google.com/bigquery/docs/write-api)
-   [Querying JSON data in BigQuery](https://cloud.google.com/bigquery/docs/reference/standard-sql/json-functions)
-   [BigQuery product documentation](https://cloud.google.com/bigquery/docs)
